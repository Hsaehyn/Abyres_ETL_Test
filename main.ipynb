{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Data Collection\n",
    "* I have selected the JSON URL and load it to the local using python\n",
    "* I stored the the JSON file as \"todos_data.json\" in the \"data\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def fetch_data(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch data from the URL: {e}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to decode JSON data: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_data(data, directory, file_name):\n",
    "    try:\n",
    "        # Create directory if it doesn't exist\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "        print(f\"Data saved to {file_path}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Failed to write data to file: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://jsonplaceholder.typicode.com/todos\"\n",
    "    file_name = \"todos_data.json\"\n",
    "    directory = \"data\"\n",
    "    \n",
    "    todos_data = fetch_data(url)\n",
    "    if todos_data:\n",
    "        save_data(todos_data, directory, file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Data Transformation\n",
    "Data Transformation is usually seperated into 2 parts -- Data Validation & Normalization\n",
    "\n",
    "#### Data Validation includes:\n",
    "1. Handle missing value: Missing values can lead to data inconsistency and affect the results of queries.\n",
    "2. Handle duplicate values: Data Duplication can violate constraints like primary keys, unique keys, and foreign keys\n",
    "3. Data Type Conversion: Ensures data of each columns is stored in the correct data type, and these data types should match the later corresponding columns in MSSQL database.\n",
    "\n",
    "\n",
    "### Normalization includes:\n",
    "1. Split data into Multiple Tables: to minimize redundancy and improve data integrity, got to determine the table structures.\n",
    "2. Determine Primary keys: Determine the primary keys of each table that is unique to each row in the table.\n",
    "3. Determine Foreign Keys: After splittting data into multiple table, we need to identify relationships between these tables and use foreign keys to enforce referential integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  id                                              title  completed\n",
      "0       1   1                                 delectus aut autem      False\n",
      "1       1   2                 quis ut nam facilis et officia qui      False\n",
      "2       1   3                                fugiat veniam minus      False\n",
      "3       1   4                                   et porro tempora       True\n",
      "4       1   5  laboriosam mollitia et enim quasi adipisci qui...      False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hx/_m156f0s2h7fhh_t5ffqzm680000gn/T/ipykernel_56866/3597807945.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load JSON data into DataFrame, take a look at te first few rows to verify the data collected is fine\n",
    "df = pd.read_json(\"./data/todos_data.json\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there is any missing columns\n",
    "print(\"Checking Missing Columns:\")\n",
    "print(df.isna().sum())\n",
    "print()\n",
    "\n",
    "# check if there is any duplications:\n",
    "print(\"Checking Duplicated Rows:\")\n",
    "print(df.duplicated().sum())\n",
    "print()\n",
    "\n",
    "# check data type \n",
    "print(\"Checking Data Types:\")\n",
    "print(df.dtypes)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Validation Result: \n",
    "1. No missing values\n",
    "2. No duplicated rows\n",
    "3. For the data type, 'title' is in \"Object\" type, a data type prior special to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert title column to string type\n",
    "df['title'] = df['title'].astype(str)\n",
    "\n",
    "# Convert completed column to int type (0 for False, 1 for True)\n",
    "df['completed'] = df['completed'].astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To adhere to the principles of database normalization, particularly the Second Normal Form (2NF), I split the dataframe into two separate entities as according to 2NF, non-key attributes should be fully dependent on the entire primary key, avoiding partial dependencies within a table.\n",
    "\n",
    "Upon examination of the original dataframe, where the assumed primary key is \"id\", it becomes apparent that the non-key attribute \"userId\" does not exhibit full dependency on the primary key. This is evident as different \"id\" values may correspond to the same \"userId\". Furthermore, there exists a partial dependency between the \"title\" and \"completed\" columns, where the uniqueness of their combination should be solely reliant on the primary key, but is currently influenced by the \"userId\".\n",
    "\n",
    "\n",
    "The dataframe is split into 2 dataframe in the table structure as shown below:\n",
    "\n",
    "Users Table:\n",
    "* Columns: userId (Primary Key), \n",
    "* [The Other user-related information that might be add in later]\n",
    "\n",
    "Tasks Table:\n",
    "* Columns:\n",
    "    * id (Primary Key)\n",
    "    * userId (Foreign Key referencing Users table)\n",
    "    * title\n",
    "    * completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, reorganize the datarame into 2 different dataframe with the proper table structure\n",
    "import pandas as pd\n",
    "\n",
    "# Users Table\n",
    "users_df = df[['userId']].drop_duplicates().reset_index(drop=True)\n",
    "print(users_df.head())\n",
    "\n",
    "# Tasks Table\n",
    "tasks_df = df[['id', 'userId', 'title', 'completed']]\n",
    "print(tasks_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Loading\n",
    "1. load the data into a MySQL database.\n",
    "2. Configure constraints like primary keys and foreign keys with their referencing table-column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframe as table into database\n",
    "from sqlalchemy import create_engine\n",
    "from config import settings\n",
    "\n",
    "DATABASE_URL = f'mysql+mysqlconnector://{settings.db_user}:{settings.db_password}@{settings.db_host}:{settings.db_port}/{settings.db_name}'\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# Load dataframe as table into MySQL database for Users\n",
    "users_df.to_sql(name='Users', con=engine, if_exists='replace', index=False)\n",
    "print(\"Users table created successfully.\")\n",
    "tasks_df.to_sql(name='Tasks', con=engine, if_exists='replace', index=False)\n",
    "print(\"Tasks table created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after loading the dataframe as table into the database, configure the primary keys and foreign keys of each table \n",
    "# so they can refer to each other\n",
    "\n",
    "import mysql.connector\n",
    "\n",
    "# Configure the connection to MySQL server\n",
    "connection = mysql.connector.connect(\n",
    "    host=settings.db_host,\n",
    "    user=settings.db_user,\n",
    "    password=settings.db_password,\n",
    "    database=settings.db_name\n",
    ")\n",
    "cursor = connection.cursor()\n",
    "\n",
    "\n",
    "# Define SQL query that we are using to set the primary keys and foreign keys\n",
    "queries = ['ALTER TABLE Users ADD PRIMARY KEY (userId)',\n",
    "         'ALTER TABLE Tasks ADD PRIMARY KEY (id)',\n",
    "         'ALTER TABLE Tasks ADD CONSTRAINT fk_user_id FOREIGN KEY (userId) REFERENCES Users(userId)'\n",
    "         ]\n",
    "# Execute queries\n",
    "for query in queries:\n",
    "    cursor.execute(query)\n",
    "\n",
    "\n",
    "cursor.close()\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Data Modelling\n",
    "\n",
    "1. For each table, display the table structure, the components are:\n",
    "    * columns\n",
    "    * primary keys\n",
    "    * foreign key & its connected column at another table\n",
    "\n",
    "2. Display a star schema of the data model as requested.\n",
    "    * I drawn the star schema in \"db_star_schema.drawio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, inspect\n",
    "\n",
    "DATABASE_URL = f'mysql+mysqlconnector://{settings.db_user}:{settings.db_password}@{settings.db_host}:{settings.db_port}/{settings.db_name}'\n",
    "engine = create_engine(DATABASE_URL)\n",
    "inspector = inspect(engine)\n",
    "\n",
    "# Get a list of table names in the database\n",
    "table_names = inspector.get_table_names()\n",
    "\n",
    "# Iterate over each table to inspect its structure\n",
    "for table_name in table_names:\n",
    "    print(f\"Table: {table_name}\")\n",
    "    \n",
    "    # Get columns information for the table\n",
    "    columns = inspector.get_columns(table_name)\n",
    "    print(\"Columns:\")\n",
    "    for column in columns:\n",
    "        print(f\"- {column['name']}: {column['type']}\")\n",
    "    \n",
    "    # Get primary key information for the table\n",
    "    primary_key =inspector.get_pk_constraint(table_name)\n",
    "    print(\"Primary Key:\", primary_key)\n",
    "    \n",
    "    # Get foreign key information for the table\n",
    "    foreign_keys = inspector.get_foreign_keys(table_name)\n",
    "    print(\"Foreign Keys:\")\n",
    "    for foreign_key in foreign_keys:\n",
    "        print(f\"- {foreign_key['constrained_columns']} references {foreign_key['referred_table']}({foreign_key['referred_columns']})\")\n",
    "        \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Data Visualization\n",
    "for this part, I am going load the data from the database, one dataframe per table (Users and Tasks), then export them as csv for visualization using Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Configure database connection\n",
    "DATABASE_URL = f'mysql+mysqlconnector://{settings.db_user}:{settings.db_password}@{settings.db_host}:{settings.db_port}/{settings.db_name}'\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# Load data from the database into dataframes, take a look at the first few rows to see if the data is fine\n",
    "table1_df = pd.read_sql_table('Users', engine)\n",
    "print(table1_df.head())\n",
    "table2_df = pd.read_sql_table('Tasks', engine)\n",
    "print(table2_df.head())\n",
    "\n",
    "# Export dataframes to .csv files\n",
    "table1_df.to_csv('Users.csv', index=False)\n",
    "table2_df.to_csv('Tasks.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
